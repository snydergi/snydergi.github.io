<!DOCTYPE html>
<html>
  <head>
	<meta charset="utf-8">

	
	<script type="text/javascript"
			src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>

	<title>Grayson Snyder</title>
	<link rel="icon" type="image/png" href="/public/images/msr-student-template-favicon.png">

	<link rel="stylesheet" href="/public/stylesheets/style.css">

	<script src="//ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
</head>


  <body>

    <div id="wrapper">

      <header>
	<nav>
		<a href="/">
			<h1>My Portfolio</h1>
		</a>
		<ul>
			<li><a href="/">Portfolio</a></li>
			<li><a href="/experience">Experience</a></li>
			<li><a href="/about/">About</a></li>
			<li><a href="/resume/">Resume</a></li>
			<li><a href="/contact/">Contact</a></li>
		</ul>
	</nav>
</header>

      <main class="project">
	<section id="contact-content">
		<img id="project-image" src="/public/images/projectImages/toastImages/toast.jpg">
		<h1 id="project-title">7-DOF Food Preparation</h1>
		<h2 id="project-date">December 12, 2024</h2>

		<h2 id="overview">Overview</h2>
<p>The goal of this project, more simply referred to as ToastBot, was to use an Emika Franka Panda robot arm to pick up a piece of bread, put it in a toaster, start the toaster, and place it on a plate.</p>
<h3 id="collaborators">Collaborators</h3>
<p>This project was done in collaboration with the following students:</p>
<ul>
  <li>Sharwin Patil (https://github.com/Sharwin24)</li>
  <li>Asa Rogers (https://github.com/asarogers)</li>
  <li>Tony Shilati (https://github.com/tony-shilati)</li>
</ul>

<h3 id="demo-video">Demo Video</h3>

<video src="/public/images/projectImages/toastImages/toastDemo.mp4" width="1000" controls=""></video>

<h3 id="moveit-api">MoveIt API</h3>
<p>Prior to the start of the ToastBot project, the group developed an API to to simply interface new ROS2 nodes with the Emika Franka Panda robot’s MoveIt package. The API consists of three main subcomponents: RobotState, ScenePlanner, and MotionPlanner. The three are abstracted within the MotionPlanningInterface, which can be imported into ROS2 nodes to make motion requests of the robot arm.</p>

<h4 id="subcomponent-breakdown">Subcomponent Breakdown</h4>
<p>The ScenePlanner component can be used to update the move group planning scene. It allows users to add and remove boxes, attach and detach collision objects from the end effector, and load multiple collision objects at once. It is capable of adding objects of different shapes and dimensions as specified by the user for each function. Use of the ScenePlanner is critical particularly in instances where the robot is grasping a solid object and must plan paths around obstacles, accounting for said object in its grasp.</p>

<p>The RobotState component performs forward and inverse kinematic caluclations for the Emika Franka Panda robot arm for use in determining current or desired poses, or the angles required to achieve them.</p>

<p>The MotionPlanner component contains the bulk of the API’s functionality. It provides utilities to plan, execute, and manage robot trajectories using joint configurations, Cartesian paths, and predefined named configurations. Additionally, it allows for control of the robot end-effector. The MotionPlanner serves as a ROS2 Action Client to he MoveGroup node running on the robot arm.</p>

<p>The key components of the API used in the ToastBot project are the RobotState and MotionPlanner.</p>

<h3 id="workspace-planning">Workspace Planning</h3>
<p>With limited workspace, designing and arranging the components of the task was an important consideration. To aid in this, we created many 3d-printed fixtures including a bread holder, toaster lever extension, plate, brush holder, and butter dish holder. Each model was designed to have a slot for an April Tag such that offsets to points of interest on the model would be quick to determine. In early stages of the project, a virtual recreation of the scene using the MoveItAPI ScenePlanner was considered, but due to the reliability of the April Tag positioning and the desired poses, collisions were not of major concern and it was ultimately unused.</p>

<h3 id="computer-vision">Computer Vision</h3>
<p>Vision was a key aspect to the success of this project. We used an Intel RealSense D435i camera mounted outside the workspace of the arm on a tripod to capture the entire scene. With the camera in place, we used April Tags to create transforms between each object in the scene and the robot.</p>

<h3 id="multi-threaded-executor">Multi-Threaded Executor</h3>



	</section>

</main>


      <footer>
	<ul>
		<li>grasnyder2001@gmail.com</li>
		<li>https://github.com/snydergi</li>
		<li>(765) 650-0970</li>
	</ul>
</footer>

    </div>

  </body>
</html>